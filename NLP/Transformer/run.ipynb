{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## before run this script, make sure you initiate submodule(dataset) by\n",
    "```git submodule update```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *original repository of dataset is (https://github.com/jungyeul/korean-parallel-corpora)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "tar_name = \"./dataset/korean-english-news-v1/korean-english-park.dev.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = tarfile.open(tar_name, \"r:gz\")\n",
    "tar.extractall()\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Corpus and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "mecab = Mecab()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_en = open(\"korean-english-park.dev.en\", \"r\").read().splitlines()\n",
    "corpus_ko = open(\"korean-english-park.dev.ko\", \"r\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_kor(kl):\n",
    "    \"\"\"make preprocessed korean morpheme list from korean sentence\n",
    "    Args:\n",
    "        kl : korean sentence\n",
    "    Return:\n",
    "        preprocessed korean morpheme list\n",
    "    \"\"\"\n",
    "    kl = kl.lower().strip()\n",
    "    kl = re.sub(r'[\" \"]+', \" \", kl)\n",
    "    kl = re.sub(r\"[^가-힣0-9?.!,¿]+\", \" \", kl)\n",
    "    kl = kl.rstrip().strip()\n",
    "    kl = mecab.morphs(kl)\n",
    "    return kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_eng(el):\n",
    "    \"\"\"make preprocessed english word list from english sentence\n",
    "    Args:\n",
    "        el : english sentence\n",
    "    Return:\n",
    "        preprocessed english word list\n",
    "    \"\"\"\n",
    "    el = el.lower().strip()\n",
    "    el = re.sub(r'[\" \"]+', \" \", el)\n",
    "    el = re.sub(r\"[^a-zA-Z0-9?.!,¿]+\", \" \", el)\n",
    "    el = el.rstrip().strip()\n",
    "    el = word_tokenize(el)\n",
    "    el = el + [\"<EOS>\"]\n",
    "    return el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tokens_eng = list(map(preprocess_eng,corpus_en))                   \n",
    "tokens_kor = list(map(preprocess_kor,corpus_ko))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_eng[0] , tokens_kor[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Vacabualry Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcnt_ko = defaultdict(lambda:0)\n",
    "for text_kor in tqdm(tokens_kor):\n",
    "    for tokens in text_kor:\n",
    "        wordcnt_ko[tokens] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcnt_en = defaultdict(lambda:0)\n",
    "for text_eng in tqdm(tokens_eng):\n",
    "    for tokens in text_eng:\n",
    "        wordcnt_en[tokens] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_kor = {k + 3: v for k,v in enumerate(wordcnt_ko.keys())}\n",
    "dict_kor[0] = \"<PAD>\"\n",
    "dict_kor[1] = \"<UNK>\"\n",
    "dict_kor[2] = \"<BOS>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_kor_inv = {v:k for k,v in dict_kor.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_kor_inv[\"<BOS>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_eng = {k + 4: v for k,v in enumerate(wordcnt_en.keys())}\n",
    "dict_eng[0] = \"<PAD>\"\n",
    "dict_eng[1] = \"<UNK>\"\n",
    "dict_eng[2] = \"<BOS>\"\n",
    "dict_eng[3] = \"<EOS>\"\n",
    "dict_eng_inv = {v:k for k,v in dict_eng.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(list(map(len,tokens_kor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(list(map(len,tokens_eng)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen_kor = max(list(map(len,tokens_kor)))\n",
    "maxlen_eng = max(list(map(len,tokens_eng))) + 1 # target language needs additional space for <BOS>\n",
    "maxlen_kor, maxlen_eng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Vocabulary to Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens2indexes_kor(tokens, maxlen=maxlen_kor):\n",
    "    ret = np.zeros([maxlen], dtype=np.int32)\n",
    "    for i in range(min(maxlen, len(tokens))):\n",
    "        if tokens[i] in dict_kor_inv:\n",
    "            ret[i] = dict_kor_inv[tokens[i]]\n",
    "        else:\n",
    "            ret[i] = dict_kor_inv[\"<UNK>\"]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_kor = list(map(tokens2indexes_kor, tokens_kor))\n",
    "array_kor = np.array(array_kor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens2indexes_eng(tokens, maxlen=maxlen_eng):\n",
    "    ret = np.zeros([maxlen], dtype=np.int32)\n",
    "    for i in range(min(maxlen, len(tokens))):\n",
    "        if tokens[i] in dict_eng_inv:\n",
    "            ret[i] = dict_eng_inv[tokens[i]]\n",
    "        else:\n",
    "            ret[i] = dict_eng_inv[\"<UNK>\"]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_eng = list(map(tokens2indexes_eng, tokens_eng))\n",
    "array_eng = np.array(array_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_eng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(array_kor, array_eng, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import Transformer\n",
    "import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = Transformer(100, 4, 0.5, maxlen_kor, maxlen_eng, len(dict_kor), len(dict_eng), 2, learning_rate=3e-3, device_name=\"gpu:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model(tf.convert_to_tensor(array_kor[:1]), tf.convert_to_tensor(array_eng[:1]), True)\n",
    "transformer_model.summary(line_length=100, positions=[.70, .80, .90, 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you get OOM Error, try reduce batch_size\n",
    "transformer_model.fit(X_train, y_train , X_val, y_val, bos_index=dict_eng_inv[\"<BOS>\"], batch_size=32, tqdm_option=\"normal\", epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start token for initial target tensor\n",
    "start_token = tf.convert_to_tensor(np.expand_dims(tokens2indexes_eng([\"<BOS>\"]), 0))\n",
    "start_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = X_train[1234:1235] # 1 batch\n",
    "seq = input_seq[0]\n",
    "[dict_kor[idx] for idx in seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_seq = y_train[1234:1235]\n",
    "seq = target_seq[0]\n",
    "\" \".join([dict_eng[idx] for idx in seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = tf.convert_to_tensor(input_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_seq_shifted = tf.pad(target_seq, [[0, 0], [1, 0]], constant_values=dict_eng_inv[\"<BOS>\"])[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto encoding check\n",
    "logit = transformer_model(input_seq, target_seq_shifted, False)\n",
    "indexes = tf.argmax(logit, axis=2).numpy()[0]\n",
    "\" \".join([dict_eng[idx] for idx in indexes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beam search\n",
    "beams = transformer_model.predict(input_seq, start_token, dict_eng_inv[\"<EOS>\"], beam_cnt=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prob, beam in beams:\n",
    "    seq = beam.numpy()[0]\n",
    "    print(\" \".join([dict_eng[idx] for idx in seq if not (idx ==2 or idx==0 or idx ==3)]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beam search\n",
    "beams = transformer_model.predict(input_seq, start_token, dict_eng_inv[\"<EOS>\"], beam_cnt=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prob, beam in beams:\n",
    "    seq = beam.numpy()[0]\n",
    "    print(\" \".join([dict_eng[idx] for idx in seq if not (idx ==2 or idx==0 or idx ==3)]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clasic36",
   "language": "python",
   "name": "classic36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
