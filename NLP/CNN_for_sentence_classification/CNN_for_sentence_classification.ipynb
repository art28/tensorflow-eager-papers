{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir, remove\n",
    "from os.path import isfile, join\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/aksdmj/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download(\"movie_reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews as mr\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide filenames by its sentiment\n",
    "# error-avoiding method\n",
    "documents = defaultdict(list)\n",
    "for i in mr.fileids():\n",
    "    documents[i.split('/')[0]].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['neg', 'pos'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1000\n"
     ]
    }
   ],
   "source": [
    "print(len(documents['pos']), len(documents['neg']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2879"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate maximum length of text\n",
    "lens = [len(mr.words(i)) for i  in mr.fileids()]\n",
    "max_num_word = max(lens)\n",
    "max_num_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39768"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the number of unique words used in all texts\n",
    "unique_words = len(set(mr.words()))\n",
    "unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mr.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'detailing'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map from index to word\n",
    "vocab_dict = {i+1:v for i,v in enumerate(set(mr.words()))}\n",
    "test_vocab = vocab_dict[10495]\n",
    "test_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for padding token\n",
    "vocab_dict[0] = \"PAD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10495"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map from word to index\n",
    "vocab_dict_inv = {v:i for i,v in vocab_dict.items()}\n",
    "vocab_dict_inv[test_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add padding\n",
    "unique_words+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from google_drive_downloader import GoogleDriveDownloader as gdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you already have pretrained wordvector weights, replace weightpath variable with a path of it\n",
    "weight_dir = os.path.join(os.path.abspath('..'),\"word2vec\")\n",
    "weight_path = os.path.join(weight_dir, \"GoogleNews-vectors-negative300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory is already exist\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(weight_dir):\n",
    "    os.makedirs(weight_dir)\n",
    "else:\n",
    "    print(\"Directory is already exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained weight is already exist\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(weight_path):\n",
    "    print(\"No pretrained weight file, start download...\")\n",
    "    gdd.download_file_from_google_drive(file_id='0B7XkCwpI5KDYNlNUTTlSS21pQmM',\n",
    "                                        dest_path=weight_path + '.gz',\n",
    "                                        unzip=False)\n",
    "    inF = gzip.open(weight_path + '.gz', 'rb')\n",
    "    outF = open(weight_path, 'wb')\n",
    "    outF.write(inF.read())\n",
    "    inF.close()\n",
    "    outF.close()\n",
    "\n",
    "    remove(w2v_path + '.gz')\n",
    "\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"pretrained weight is already exist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained vector\n",
    "w2v = KeyedVectors.load_word2vec_format(weight_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39769, 300)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make w2v matrix for our dataset's words\n",
    "weights = np.array([w2v[v] if v in w2v else np.zeros(w2v.vector_size) for i ,v in vocab_dict.items()])\n",
    "# (number of words, dimension of wordvectors)\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words2indexs(words):\n",
    "    return np.array([vocab_dict_inv[word] for word in words])\n",
    "\n",
    "# make index array with size of (max_num_word), with 0(\"PAD\" word) padding\n",
    "def preprocess(document):\n",
    "    indexs = words2indexs(mr.words(document))\n",
    "    return np.concatenate([indexs, np.zeros([max_num_word - indexs.shape[0]], dtype=\"int64\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list()\n",
    "y = list()\n",
    "\n",
    "# 0 label for negative , 1 for positive\n",
    "for i in documents['neg']:\n",
    "    tx = preprocess(i)\n",
    "    X.append(tx)\n",
    "    y.append(0)\n",
    "    \n",
    "for i in documents['pos']:\n",
    "    tx = preprocess(i)\n",
    "    X.append(tx)\n",
    "    y.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "tfe.enable_eager_execution(device_policy=tfe.DEVICE_PLACEMENT_SILENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn_sentence import CNN_classification_single, CNN_classification_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'gpu:0' if tfe.num_gpus() > 0 else 'cpu:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_classfication_single_static = CNN_classification_single(\n",
    "    num_words=unique_words, in_dim=max_num_word, out_dim=2, is_static=True, device_name=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_classfication_single_static.copy_pretrained(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  11930700  \n",
      "_________________________________________________________________\n",
      "conv11 (Conv1D)              multiple                  90100     \n",
      "_________________________________________________________________\n",
      "conv12 (Conv1D)              multiple                  120100    \n",
      "_________________________________________________________________\n",
      "conv13 (Conv1D)              multiple                  150100    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "out (Dense)                  multiple                  602       \n",
      "=================================================================\n",
      "Total params: 12,291,602\n",
      "Trainable params: 360,902\n",
      "Non-trainable params: 11,930,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_classfication_single_static(tf.convert_to_tensor(X_train[:1]), True)\n",
    "cnn_classfication_single_static.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN   1: 100%|██████████| 50/50 [00:12<00:00,  9.85it/s]\n",
      "VAL     1: 100%|██████████| 13/13 [00:10<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m=========================\n",
      "[EPOCH 1 / STEP 1]\n",
      "TRAIN loss   : 24.3521\n",
      "VAL   loss   : 20.8941\n",
      "VAL   acc    : 69.7500%\n",
      "=========================\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN   2: 100%|██████████| 50/50 [00:14<00:00,  3.47it/s]\n",
      "VAL     2: 100%|██████████| 13/13 [00:10<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m=========================\n",
      "[EPOCH 2 / STEP 2]\n",
      "TRAIN loss   : 16.9989\n",
      "VAL   loss   : 19.4675\n",
      "VAL   acc    : 75.0000%\n",
      "=========================\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN   3: 100%|██████████| 50/50 [00:11<00:00,  9.86it/s]\n",
      "VAL     3: 100%|██████████| 13/13 [00:10<00:00,  1.28s/it]\n",
      "TRAIN   4: 100%|██████████| 50/50 [00:14<00:00,  3.45it/s]\n",
      "VAL     4: 100%|██████████| 13/13 [00:10<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m=========================\n",
      "[EPOCH 4 / STEP 4]\n",
      "TRAIN loss   : 9.4260\n",
      "VAL   loss   : 17.8212\n",
      "VAL   acc    : 78.2500%\n",
      "=========================\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN   5: 100%|██████████| 50/50 [00:11<00:00, 10.02it/s]\n",
      "VAL     5: 100%|██████████| 13/13 [00:10<00:00,  1.27s/it]\n",
      "TRAIN   6: 100%|██████████| 50/50 [00:11<00:00,  9.94it/s]\n",
      "VAL     6: 100%|██████████| 13/13 [00:10<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m=========================\n",
      "[EPOCH 6 / STEP 6]\n",
      "TRAIN loss   : 5.7961\n",
      "VAL   loss   : 16.5816\n",
      "VAL   acc    : 79.0000%\n",
      "=========================\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN   7: 100%|██████████| 50/50 [00:11<00:00,  9.87it/s]\n",
      "VAL     7: 100%|██████████| 13/13 [00:10<00:00,  1.28s/it]\n",
      "TRAIN   8: 100%|██████████| 50/50 [00:11<00:00,  9.82it/s]\n",
      "VAL     8: 100%|██████████| 13/13 [00:10<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m=========================\n",
      "[EPOCH 8 / STEP 8]\n",
      "TRAIN loss   : 4.0197\n",
      "VAL   loss   : 15.9876\n",
      "VAL   acc    : 80.5000%\n",
      "=========================\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN   9: 100%|██████████| 50/50 [00:11<00:00,  9.79it/s]\n",
      "VAL     9: 100%|██████████| 13/13 [00:10<00:00,  1.28s/it]\n",
      "TRAIN  10: 100%|██████████| 50/50 [00:11<00:00,  9.87it/s]\n",
      "VAL    10: 100%|██████████| 13/13 [00:10<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m=========================\n",
      "[EPOCH 10 / STEP 10]\n",
      "TRAIN loss   : 3.3394\n",
      "VAL   loss   : 15.8101\n",
      "VAL   acc    : 80.0000%\n",
      "=========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# if you get OOM error, use smaller batch_size\n",
    "cnn_classfication_single_static.fit(X_train, y_train, X_val, y_val, batch_size =32, epochs=10, verbose=2, tqdm_option=\"normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      multiple                  11930700  \n",
      "_________________________________________________________________\n",
      "conv11 (Conv1D)              multiple                  90100     \n",
      "_________________________________________________________________\n",
      "conv12 (Conv1D)              multiple                  120100    \n",
      "_________________________________________________________________\n",
      "conv13 (Conv1D)              multiple                  150100    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "out (Dense)                  multiple                  602       \n",
      "=================================================================\n",
      "Total params: 12,291,602\n",
      "Trainable params: 12,291,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_classfication_single_non_static = CNN_classification_single(\n",
    "    num_words=unique_words, in_dim=max_num_word, out_dim=2, is_static=False, device_name=device)\n",
    "cnn_classfication_single_non_static.copy_pretrained(weights)\n",
    "cnn_classfication_single_non_static(tf.convert_to_tensor(X_train[:1]), True)\n",
    "cnn_classfication_single_non_static.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN   1: 100%|██████████| 50/50 [00:24<00:00,  2.78it/s]\n",
      "VAL     1: 100%|██████████| 13/13 [00:10<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m=========================\n",
      "[EPOCH 1 / STEP 1]\n",
      "TRAIN loss   : 23.0523\n",
      "VAL   loss   : 20.5473\n",
      "VAL   acc    : 66.0000%\n",
      "=========================\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN   2: 100%|██████████| 50/50 [00:24<00:00,  2.82it/s]\n",
      "VAL     2: 100%|██████████| 13/13 [00:10<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m=========================\n",
      "[EPOCH 2 / STEP 2]\n",
      "TRAIN loss   : 13.9169\n",
      "VAL   loss   : 19.3215\n",
      "VAL   acc    : 70.7500%\n",
      "=========================\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN   3: 100%|██████████| 50/50 [00:25<00:00,  2.80it/s]\n",
      "VAL     3: 100%|██████████| 13/13 [00:10<00:00,  1.31s/it]\n",
      "TRAIN   4: 100%|██████████| 50/50 [00:24<00:00,  2.83it/s]\n",
      "VAL     4: 100%|██████████| 13/13 [00:10<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m=========================\n",
      "[EPOCH 4 / STEP 4]\n",
      "TRAIN loss   : 5.3293\n",
      "VAL   loss   : 17.0537\n",
      "VAL   acc    : 79.0000%\n",
      "=========================\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN   5: 100%|██████████| 50/50 [00:24<00:00,  2.69it/s]\n",
      "VAL     5: 100%|██████████| 13/13 [00:10<00:00,  1.29s/it]\n",
      "TRAIN   6: 100%|██████████| 50/50 [00:24<00:00,  2.80it/s]\n",
      "VAL     6: 100%|██████████| 13/13 [00:10<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m=========================\n",
      "[EPOCH 6 / STEP 6]\n",
      "TRAIN loss   : 3.3064\n",
      "VAL   loss   : 16.2035\n",
      "VAL   acc    : 79.7500%\n",
      "=========================\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN   7: 100%|██████████| 50/50 [00:24<00:00,  2.78it/s]\n",
      "VAL     7: 100%|██████████| 13/13 [00:10<00:00,  1.31s/it]\n",
      "TRAIN   8: 100%|██████████| 50/50 [00:24<00:00,  2.80it/s]\n",
      "VAL     8: 100%|██████████| 13/13 [00:10<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m=========================\n",
      "[EPOCH 8 / STEP 8]\n",
      "TRAIN loss   : 2.6412\n",
      "VAL   loss   : 15.6427\n",
      "VAL   acc    : 82.7500%\n",
      "=========================\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN   9: 100%|██████████| 50/50 [00:24<00:00,  2.82it/s]\n",
      "VAL     9: 100%|██████████| 13/13 [00:10<00:00,  1.32s/it]\n",
      "TRAIN  10: 100%|██████████| 50/50 [00:25<00:00,  2.74it/s]\n",
      "VAL    10: 100%|██████████| 13/13 [00:10<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m=========================\n",
      "[EPOCH 10 / STEP 10]\n",
      "TRAIN loss   : 2.2993\n",
      "VAL   loss   : 15.2547\n",
      "VAL   acc    : 80.7500%\n",
      "=========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# if you get OOM error, use smaller batch_size\n",
    "cnn_classfication_single_non_static.fit(X_train, y_train, X_val, y_val, batch_size =32, epochs=10, verbose=2, tqdm_option=\"normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "w2v_trainable (Embedding)    multiple                  11930700  \n",
      "_________________________________________________________________\n",
      "w2v_nontrainable (Embedding) multiple                  11930700  \n",
      "_________________________________________________________________\n",
      "conv11 (Conv1D)              multiple                  90100     \n",
      "_________________________________________________________________\n",
      "conv12 (Conv1D)              multiple                  120100    \n",
      "_________________________________________________________________\n",
      "conv13 (Conv1D)              multiple                  150100    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "out (Dense)                  multiple                  602       \n",
      "=================================================================\n",
      "Total params: 24,222,302\n",
      "Trainable params: 12,291,602\n",
      "Non-trainable params: 11,930,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_classfication_multi = CNN_classification_multi(\n",
    "    num_words=unique_words, in_dim=max_num_word, out_dim=2, device_name=device)\n",
    "cnn_classfication_multi.copy_pretrained(weights)\n",
    "cnn_classfication_multi(tf.convert_to_tensor(X_train[:1]), True)\n",
    "cnn_classfication_multi.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN   1: 100%|██████████| 50/50 [00:30<00:00,  2.13it/s]\n",
      "VAL     1: 100%|██████████| 13/13 [00:11<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m=========================\n",
      "[EPOCH 1 / STEP 1]\n",
      "TRAIN loss   : 27.0469\n",
      "VAL   loss   : 19.7353\n",
      "VAL   acc    : 68.7500%\n",
      "=========================\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN   2: 100%|██████████| 50/50 [00:30<00:00,  2.12it/s]\n",
      "VAL     2: 100%|██████████| 13/13 [00:11<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m=========================\n",
      "[EPOCH 2 / STEP 2]\n",
      "TRAIN loss   : 11.4859\n",
      "VAL   loss   : 18.4603\n",
      "VAL   acc    : 76.0000%\n",
      "=========================\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN   3: 100%|██████████| 50/50 [00:30<00:00,  2.16it/s]\n",
      "VAL     3: 100%|██████████| 13/13 [00:11<00:00,  1.09s/it]\n",
      "TRAIN   4: 100%|██████████| 50/50 [00:30<00:00,  2.08it/s]\n",
      "VAL     4: 100%|██████████| 13/13 [00:11<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m=========================\n",
      "[EPOCH 4 / STEP 4]\n",
      "TRAIN loss   : 4.6140\n",
      "VAL   loss   : 16.9017\n",
      "VAL   acc    : 79.5000%\n",
      "=========================\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN   5: 100%|██████████| 50/50 [00:30<00:00,  2.16it/s]\n",
      "VAL     5: 100%|██████████| 13/13 [00:11<00:00,  1.06s/it]\n",
      "TRAIN   6: 100%|██████████| 50/50 [00:30<00:00,  2.08it/s]\n",
      "VAL     6: 100%|██████████| 13/13 [00:11<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m=========================\n",
      "[EPOCH 6 / STEP 6]\n",
      "TRAIN loss   : 3.2240\n",
      "VAL   loss   : 15.9325\n",
      "VAL   acc    : 81.0000%\n",
      "=========================\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN   7: 100%|██████████| 50/50 [00:30<00:00,  2.14it/s]\n",
      "VAL     7: 100%|██████████| 13/13 [00:11<00:00,  1.08s/it]\n",
      "TRAIN   8: 100%|██████████| 50/50 [00:30<00:00,  2.11it/s]\n",
      "VAL     8: 100%|██████████| 13/13 [00:11<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m=========================\n",
      "[EPOCH 8 / STEP 8]\n",
      "TRAIN loss   : 2.6693\n",
      "VAL   loss   : 15.4559\n",
      "VAL   acc    : 79.2500%\n",
      "=========================\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN   9: 100%|██████████| 50/50 [00:30<00:00,  2.12it/s]\n",
      "VAL     9: 100%|██████████| 13/13 [00:11<00:00,  1.02it/s]\n",
      "TRAIN  10: 100%|██████████| 50/50 [00:27<00:00,  2.41it/s]\n",
      "VAL    10: 100%|██████████| 13/13 [00:11<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m=========================\n",
      "[EPOCH 10 / STEP 10]\n",
      "TRAIN loss   : 2.3960\n",
      "VAL   loss   : 15.2340\n",
      "VAL   acc    : 80.5000%\n",
      "=========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# if you get OOM error, use smaller batch_size\n",
    "cnn_classfication_multi.fit(X_train, y_train, X_val, y_val, batch_size =32, epochs=10, verbose=2, tqdm_option=\"normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clasic36",
   "language": "python",
   "name": "classic36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
